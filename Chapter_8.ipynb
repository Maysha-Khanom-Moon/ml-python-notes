{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a585f56b",
   "metadata": {},
   "source": [
    "## Feature Selection For Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bc17edb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bdfcb496",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('dataset/diabetes.csv')\n",
    "X = data.iloc[:, 0:8].values\n",
    "y = data.iloc[:, 8].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71719541",
   "metadata": {},
   "source": [
    "<b>Feature Selection: </b> <br>\n",
    "A process where you automatically select those features in your data that contribute most to the prediction variable\n",
    "- Reduces Overfitting\n",
    "- Improves Accuracy\n",
    "- Reduces Training Time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bc2fb2",
   "metadata": {},
   "source": [
    "### Univariate Selection\n",
    "Select those features that have the strongest relationship with the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8517b8d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[148. ,   0. ,  33.6,  50. ],\n",
       "       [ 85. ,   0. ,  26.6,  31. ],\n",
       "       [183. ,   0. ,  23.3,  32. ],\n",
       "       ...,\n",
       "       [121. , 112. ,  26.2,  30. ],\n",
       "       [126. ,   0. ,  30.1,  47. ],\n",
       "       [ 93. ,   0. ,  30.4,  23. ]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "# chi2 test for non-negative values\n",
    "selector = SelectKBest(score_func=chi2, k=4)\n",
    "X_new = selector.fit_transform(X, y)\n",
    "np.set_printoptions(precision=3)\n",
    "\n",
    "X_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e0e904",
   "metadata": {},
   "source": [
    "### Recursive Feature Elimination\n",
    "RFA works by recursively removing attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e36682d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Features: 3\n",
      "Selected Features: [ True False False False False  True  True False]\n",
      "Feature Ranking: [1 2 4 5 6 1 1 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression()\n",
    "rfe = RFE(estimator=model, n_features_to_select=3)\n",
    "fit = rfe.fit(X, y)\n",
    "\n",
    "print(f'Num Features: {fit.n_features_}')\n",
    "print(f'Selected Features: {fit.support_}')\n",
    "print(f'Feature Ranking: {fit.ranking_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585f7018",
   "metadata": {},
   "source": [
    "### Principal Component Analysis\n",
    "Transform the dataset into a compressed form. It's called data reduction technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "800c88c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained Variance: [0.889 0.062 0.026]\n",
      "[[-2.022e-03  9.781e-02  1.609e-02  6.076e-02  9.931e-01  1.401e-02\n",
      "   5.372e-04 -3.565e-03]\n",
      " [ 2.265e-02  9.722e-01  1.419e-01 -5.786e-02 -9.463e-02  4.697e-02\n",
      "   8.168e-04  1.402e-01]\n",
      " [ 2.246e-02 -1.434e-01  9.225e-01  3.070e-01 -2.098e-02  1.324e-01\n",
      "   6.400e-04  1.255e-01]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "fit = pca.fit(X)\n",
    "\n",
    "# how much variance each principle component explains\n",
    "# PCA keeps the components with the most variance. These components explain the most important patterns in the data\n",
    "print(f'Explained Variance: {fit.explained_variance_ratio_}')\n",
    "\n",
    "# how much each feature contributes to each principal component\n",
    "print(fit.components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ce35bf",
   "metadata": {},
   "source": [
    "### Feature Importance\n",
    "Gives an importance score for each attribute where the larger the score, the more important the attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eeb01aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.11  0.236 0.1   0.076 0.076 0.144 0.118 0.142]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "model = ExtraTreesClassifier()\n",
    "model.fit(X, y)\n",
    "print(model.feature_importances_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
